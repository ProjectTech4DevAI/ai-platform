import logging

from fastapi import APIRouter, Body, Depends, UploadFile, File, Form
from sqlmodel import Session, select

from app.api.deps import get_current_user_org_project, get_db
from app.core.util import configure_langfuse, configure_openai, now
from app.crud.credentials import get_provider_credential
from app.crud.evaluation import upload_dataset_to_langfuse
from app.crud.evaluation_batch import start_evaluation_batch
from app.crud.evaluation_processing import poll_all_pending_evaluations
from app.crud.assistants import get_assistant_by_id
from app.models import UserProjectOrg, EvaluationRun
from app.models.evaluation import (
    DatasetUploadResponse,
    EvaluationRunPublic,
)

logger = logging.getLogger(__name__)

router = APIRouter(tags=["evaluation"])


@router.post("/dataset/upload", response_model=DatasetUploadResponse)
async def upload_dataset(
    file: UploadFile = File(
        ..., description="CSV file with 'question' and 'answer' columns"
    ),
    dataset_name: str = Form(..., description="Name for the dataset in Langfuse"),
    duplication_factor: int = Form(
        default=5, description="Number of times to duplicate each item"
    ),
    _session: Session = Depends(get_db),
    _current_user: UserProjectOrg = Depends(get_current_user_org_project),
) -> DatasetUploadResponse:
    """
    Upload a CSV file containing Golden Q&A pairs to Langfuse as a dataset.
    Each question will be duplicated N times (default 5) to test LLM flakiness.

    CSV Format:
    - Must contain 'question' and 'answer' columns
    - Can have additional columns (will be ignored)

    Example CSV:
    ```
    question,answer
    "What is the capital of France?","Paris"
    "What is 2+2?","4"
    ```
    """
    logger.info(
        f"Uploading dataset: {dataset_name} with duplication factor: {duplication_factor}"
    )

    # Read CSV content
    content = await file.read()

    success, data, error = await upload_dataset_to_langfuse(
        csv_content=content,
        dataset_name=dataset_name,
        duplication_factor=duplication_factor,
        _session=_session,
        _current_user=_current_user,
    )

    if not success or data is None:
        raise ValueError(error or "Failed to upload dataset")

    logger.info(
        f"Successfully uploaded dataset: {dataset_name} with {data.total_items} items "
        f"({data.original_items} original items Ã— {duplication_factor})"
    )

    return data


@router.post("/evaluate", response_model=EvaluationRunPublic)
async def evaluate_threads(
    dataset_name: str = Body(..., description="Name of the Langfuse dataset"),
    experiment_name: str = Body(
        ..., description="Name for this evaluation experiment/run"
    ),
    config: dict = Body(..., description="Evaluation configuration"),
    _session: Session = Depends(get_db),
    _current_user: UserProjectOrg = Depends(get_current_user_org_project),
) -> EvaluationRunPublic:
    """
    Start an evaluation using OpenAI Batch API.

    This endpoint:
    1. Creates an EvaluationRun record in the database
    2. Fetches dataset items from Langfuse
    3. Builds JSONL for OpenAI Batch API (using provided config)
    4. Uploads to OpenAI and creates batch job
    5. Returns the evaluation run details with batch_id

    The batch will be processed asynchronously by Celery Beat (every 60s).
    Use GET /evaluate/batch/{run_id}/status to check progress.

    Args:
        dataset_name: Name of the Langfuse dataset
        experiment_name: Name for this evaluation experiment/run
        config: Configuration dict with optional fields:
            - assistant_id (optional): If provided, fetch config from openai_assistant table
            - llm (optional): {"model": "gpt-4o", "temperature": 0.2}
            - instructions (optional): System instructions
            - vector_store_ids (optional): List of vector store IDs

    Example config:
    {
        "llm": {"model": "gpt-4o", "temperature": 0.2},
        "instructions": "You are a friendly assistant",
        "vector_store_ids": ["vs_abc123"],
        "assistant_id": "asst_xyz"  # Optional - fetches from DB if provided
    }

    Returns:
        EvaluationRunPublic with batch details and status
    """
    logger.info(
        f"Starting evaluation: experiment_name={experiment_name}, "
        f"dataset={dataset_name}, "
        f"org_id={_current_user.organization_id}, "
        f"config_keys={list(config.keys())}"
    )

    # Get credentials
    openai_credentials = get_provider_credential(
        session=_session,
        org_id=_current_user.organization_id,
        provider="openai",
    )
    langfuse_credentials = get_provider_credential(
        session=_session,
        org_id=_current_user.organization_id,
        provider="langfuse",
    )

    if not openai_credentials or not langfuse_credentials:
        raise ValueError("OpenAI or Langfuse credentials not configured")

    # Configure clients
    openai_client, openai_success = configure_openai(openai_credentials)
    langfuse, langfuse_success = configure_langfuse(langfuse_credentials)

    if not openai_success or not langfuse_success:
        raise ValueError("Failed to configure API clients")

    # Check if assistant_id is provided in config
    assistant_id = config.get("assistant_id")
    if assistant_id:
        # Fetch assistant details from database
        assistant = get_assistant_by_id(
            session=_session,
            assistant_id=assistant_id,
            project_id=_current_user.project_id,
        )

        if assistant:
            logger.info(
                f"Found assistant in DB: id={assistant.id}, "
                f"model={assistant.model}, instructions={assistant.instructions[:50]}..."
            )

            # Merge DB config with provided config (provided config takes precedence)
            db_config = {
                "assistant_id": assistant_id,
                "llm": {
                    "model": assistant.model,
                    "temperature": assistant.temperature,
                },
                "instructions": assistant.instructions,
                "vector_store_ids": assistant.vector_store_ids or [],
            }

            # Override with provided config values
            for key in ["llm", "instructions", "vector_store_ids"]:
                if key in config:
                    db_config[key] = config[key]

            config = db_config
            logger.info(f"Using merged config from DB and provided values")
        else:
            logger.warning(
                f"Assistant {assistant_id} not found in DB, using provided config"
            )
    else:
        logger.info("No assistant_id provided, using provided config directly")

    # Ensure config has required fields with defaults
    if "llm" not in config:
        config["llm"] = {"model": "gpt-4o", "temperature": 0.2}
    if "instructions" not in config:
        config["instructions"] = "You are a helpful assistant"
    if "vector_store_ids" not in config:
        config["vector_store_ids"] = []

    # Create EvaluationRun record
    eval_run = EvaluationRun(
        run_name=experiment_name,
        dataset_name=dataset_name,
        config=config,
        status="pending",
        organization_id=_current_user.organization_id,
        project_id=_current_user.project_id,
        inserted_at=now(),
        updated_at=now(),
    )

    _session.add(eval_run)
    _session.commit()
    _session.refresh(eval_run)

    logger.info(f"Created EvaluationRun record: id={eval_run.id}")

    # Start the batch evaluation
    try:
        eval_run = start_evaluation_batch(
            langfuse=langfuse,
            openai_client=openai_client,
            session=_session,
            eval_run=eval_run,
            config=config,
        )

        logger.info(
            f"Evaluation started successfully: "
            f"batch_id={eval_run.batch_id}, total_items={eval_run.total_items}"
        )

        return eval_run

    except Exception as e:
        logger.error(
            f"Failed to start evaluation for run {eval_run.id}: {e}",
            exc_info=True,
        )
        # Error is already handled in start_evaluation_batch
        _session.refresh(eval_run)
        return eval_run


@router.post("/evaluate/batch/poll")
async def poll_evaluation_batches(
    _session: Session = Depends(get_db),
    _current_user: UserProjectOrg = Depends(get_current_user_org_project),
) -> dict:
    """
    Manually trigger polling for all pending evaluations in the current organization.

    This endpoint is useful for:
    - Testing the evaluation flow
    - Immediately checking status instead of waiting for Celery beat
    - Debugging evaluation issues

    Returns:
        Summary of polling results including processed, failed, and still processing counts
    """
    logger.info(
        f"Manual polling triggered for org_id={_current_user.organization_id} "
        f"by user_id={_current_user.user_id}"
    )

    summary = await poll_all_pending_evaluations(
        session=_session, org_id=_current_user.organization_id
    )

    logger.info(
        f"Manual polling completed for org_id={_current_user.organization_id}: "
        f"{summary.get('total', 0)} evaluations checked, "
        f"{summary.get('processed', 0)} processed, "
        f"{summary.get('failed', 0)} failed"
    )

    return summary


@router.get("/evaluate/batch/{run_id}/status", response_model=EvaluationRunPublic)
async def get_evaluation_run_status(
    run_id: int,
    _session: Session = Depends(get_db),
    _current_user: UserProjectOrg = Depends(get_current_user_org_project),
) -> EvaluationRunPublic:
    """
    Get the current status of a specific evaluation run.

    Args:
        run_id: ID of the evaluation run

    Returns:
        EvaluationRunPublic with current status, batch_status, and results if completed
    """
    logger.info(
        f"Fetching status for evaluation run {run_id} "
        f"(org_id={_current_user.organization_id})"
    )

    # Query the evaluation run
    statement = (
        select(EvaluationRun)
        .where(EvaluationRun.id == run_id)
        .where(EvaluationRun.organization_id == _current_user.organization_id)
    )

    eval_run = _session.exec(statement).first()

    if not eval_run:
        raise ValueError(
            f"Evaluation run {run_id} not found or not accessible to this organization"
        )

    logger.info(
        f"Found evaluation run {run_id}: status={eval_run.status}, "
        f"batch_status={eval_run.batch_status}"
    )

    return eval_run


@router.get("/evaluate/batch/list", response_model=list[EvaluationRunPublic])
async def list_evaluation_runs(
    _session: Session = Depends(get_db),
    _current_user: UserProjectOrg = Depends(get_current_user_org_project),
    limit: int = 50,
    offset: int = 0,
) -> list[EvaluationRunPublic]:
    """
    List all evaluation runs for the current organization.

    Args:
        limit: Maximum number of runs to return (default 50)
        offset: Number of runs to skip (for pagination)

    Returns:
        List of EvaluationRunPublic objects, ordered by most recent first
    """
    logger.info(
        f"Listing evaluation runs for org_id={_current_user.organization_id} "
        f"(limit={limit}, offset={offset})"
    )

    statement = (
        select(EvaluationRun)
        .where(EvaluationRun.organization_id == _current_user.organization_id)
        .order_by(EvaluationRun.inserted_at.desc())
        .limit(limit)
        .offset(offset)
    )

    runs = _session.exec(statement).all()

    logger.info(f"Found {len(runs)} evaluation runs")

    return list(runs)
